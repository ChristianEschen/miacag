# use a fixed random seed to guarantee that when you run the code twice you will get the same outcome
manual_seed: 0
# PyTorch configuration
local_rank: 1


# model configuration
model:
# model class, e.g. UNet
  dimension: 2D+T
  encoder_depth: 4
  in_channels: 1
  backbone: r3d_18
  num_classes: 2
  pretrained: False
  pretrain_model: 'None'
  pretrain_encoder: 'None'
# Data loader configuration
task_type: "classification"
labels_dict: {0: 0,
          1: 1,
          2: 2,
          3: 4,
          4: 5,
          6: 6,
          7: 6,
          8: 6,
          9: 6,
          10: 6,
          11: 6,
          12: 6,
          13: 6,
          14: 6,
          15: 6,
          16: 6,
          17: 6,
          18: 6,
          19: 6,
          20: 6}
loaders:
  #dimension order
  format: 'dicom'
  store_memory: False

  use_amp: False
  

  CropForeGround: False
  isCT: False

  spatial_resize: False
  pixdim_height: 2 # 0.213828125
  pixdim_width: 2 # 0.213828125
  pixdim_depth: 0.33 # 0.33 # 1/66.673
  Resize_height: 112 # 224
  Resize_width: 112 # 224
  Resize_depth: -1


  Crop_height: 112 # 224
  Crop_width: 112 # 224
  Crop_depth: 32

  batchSize: 2


  # validation method 
  val_method:
    type: "patches" # patches | sliding_window
    samples: 1

# trainer configuration
trainer:
  # how many iterations between validations
  validate_frequency: 1
  # max number of epochs
  epochs: 1
  # number of epochs before use early stopping
  max_stagnation: 100

# optimizer configuration
optimizer:
  type: "sgd" # 
  # initial learning rate
  learning_rate: 0.01 # 0.05*BS/256 -> 0.1 if BS=512
  # weight decay
  weight_decay: 0.0005 #5e-4 
  momentum: 0.9

lr_scheduler:
  type: False # NOT IMPLEMENTED JET
  lr_warmup_epochs: 10
  gamma: 0.2
# loss function configuration
loss:
  # loss function to be used during training
  name: ['CE'] #CE_pixel CE_pixel
# evaluation metric configuration
eval_metric_train:
  name: ['acc_top_1']
eval_metric_val:
  name: ['acc_top_1']
# learning rate scheduler configuration