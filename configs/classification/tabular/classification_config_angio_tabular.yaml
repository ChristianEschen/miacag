# use a fixed random seed to guarantee that when you run the code twice you will get the same outcome
manual_seed: 0

# model configuration
model:
# model class, e.g. UNet
  dimension: tabular
  encoder_depth: 4
  in_channels: 1
  backbone: mlp ##linear #r2plus1d_18 #x3d_s #x3d_l #r2plus1d_18 # x3d_l
  num_classes: 2
  pretrained: False
  pretrain_model: "None"
  pretrain_encoder: "None"
  incomming_features: 4
# Data loader configuration
task_type: "classification"
labels_dict: {0: 0,
          1: 1,
          2: 2,
          3: 3,
          4: 4,
          6: 6,
          7: 6,
          8: 6,
          9: 6,
          10: 6,
          11: 6,
          12: 6,
          13: 6,
          14: 6,
          15: 6,
          16: 6,
          17: 6,
          18: 6,
          19: 6,
          20: 6}
cache_num: 'None'
cache_rate: 0.001 #0.25
loaders:
  #dimension order
  store_memory: False
  format: 'db'
  use_amp: True
  batchSize: 4
  # validation method 
  val_method:
    type: full #'sliding_window' # patches | sliding_window | f
    samples: 1
    saliency: 'False'
    misprediction: 'False'

# trainer configuration
trainer:
  # how many iterations between validations
  validate_frequency: 9999
  # max number of epochs
  epochs: 10
  # number of epochs before use early stopping
  max_stagnation: 100

# optimizer configuration
optimizer:
  type: "sgd" # 
  # initial learning rate
  learning_rate: 0.10 # 0.0003125 # 0.05*BS/256 -> 0.1 if BS=512
  # weight decay
  weight_decay: 0.0005 #5e-4 
  momentum: 0.9

lr_scheduler:
  type: None #
  lr_warmup_epochs: 10
  steps_for_drop: 1
  gamma: 0.2
# loss function configuration
loss:
  # loss function to be used during training
  name: ['CE'] #CE_pixel CE_pixel
# evaluation metric configuration
eval_metric_train:
  name: ['acc_top_1']
eval_metric_val:
  name: ['acc_top_1']
# learning rate scheduler configuration
